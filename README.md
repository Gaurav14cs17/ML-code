# ML-code


## Option 2: Choose what you want from here: 
## Chapter 2 (Preliminaries):
## Plots a 1D Gaussian (Fig 2.14)
## Plot some 2D Gaussians
## Chapter 3 (Neurons, Neural Networks, and Linear Discriminants):
## The Perceptron
## The Linear Regressor
## Another Perceptron (for use with logic.py)
## Demonstration of Perceptron with logic functions
## Demonstration of Linear Regressor with logic functions
## Demonstration of Perceptron with Pima Indian dataset
## Demonstration of Linear Regressor with auto-mpg dataset
## Demonstration of Perceptron with the MNIST dataset
## Chapter 4 (The Multi-Layer Perceptron):
## The Multi-Layer Perceptron
## Demonstration of the MLP on logic functions
## Demonstration of the MLP for classification on the Iris dataset
## Demonstration of the MLP for regression on data from a sine wave
## Demonstration of the MLP for time series on the Palmerston North Ozone dataset
## Demonstration of MLP with the MNIST dataset
## Chapter 5 (Radial Basis Functions and Splines):
## The Radial Basis Function
## Linear Least Squares Fitting
## Demonstration of the RBF on the Iris dataset
## Chapter 6 (Dimensionality Reduction):
## Linear Discriminant Analysis
## Principal Components Analysis
## Factor Analysis
##Kernel Principal Components Analysis
##Locally Linear Embedding
##Isomap
##Demonstration of PCA
##Demonstration of kernel PCA on a set of circular data
##Demonstration of the algorithms on the Iris dataset
##Chapter 7 (Probabilistic Learning):
##The Gaussian Mixture Model
##The k-Nearest Neighbour Algorithm
##The k-Nearest Neighbour Smoother
##The kd-Tree Algorithm
##Chapter 8 (Support Vector Machines):
##The Support Vector Machine (needs cvxopt)
##Demonstration of the SVM for classification on the Iris dataset
## Demonstration of the SVM for the variant of XOR in Figs 8.7 and 8.8
## Chapter 9 (Optimisation and Search):
## Steepest Descent
## Newton's method
## Levenberg-Marquarft
## Conjugate Gradients
## The version of the MLP algorithm trained using conjugate gradients
## Demonstration of the MLP algorithm trained using conjugate gradients on the Iris dataset
## Demonstration of Levenberg-Marquardt on a least-squares fitting problem
## Demonstration of four solution methods for the Travelling Salesman Problem
## Chapter 10 (Evolutionary Learning):
## The Genetic Algorithm
## A Runner for the GA
## Population-Based Incremental Learning
## A knapsack problem fitness function
## The four peaks fitness function
## The onemax fitness function
## Exhaustive search algorithm to solve the knapsack problem
## A greedy algorithm to solve the knapsack problem
## Chapter 11 (Reinforcement Learning):
## The SARSA algorithm
## The TD(0) algorithm
## Demonstration of the SARSA algorithm on the Cliff problem
## Demonstration of the TD(0) algorithm on the Cliff problem
## Chapter 12 (Learning with Trees):
## The decision tree
## Demonstration of the decision tree on the Party dataset
## The Party dataset
##Chapter 13 (Decision by Committee: Ensemble Learning):
##The Boosting algorithm
##The Bagging algorithm
##A decision tree with weights
##The random forest algorithm
##Demonstration of bagging, stumping and random forests on the Party dataset
##Demonstration of bagging on the Car Safety dataset
##Demonstration of bagging and random forests on the mushroom dataset
##Chapter 14 (Unsupervised Learning):
##The k-Means Algorithm
##The k-Means Neural Network
##The Self-Organising Map
##A Simple 2D Demonstration of the SOM
##Demonstration of k-Means and the SOM on the Iris dataset
## Demonstration of SOM and Perceptron together on the Iris dataset
## More demonstrations of the SOM
## Chapter 15 (Markov Chain Monte Carlo Methods):
## The Linear Congruential Pseudo-Random Number Generator
## The Box-Muller method of constructing Gaussian-distributed pseudo-random numbers
## The Rejection Sampling Algorithm
## The Importance Sampling Algorithm
## The Sampling-Importance-Resampling Algorithm
## The Metropolis-Hastings Algorithm
## The Gibbs Sampler
## Chapter 16 (Graphical Models):
## The Gibbs Sampler for the Exam Panic dataset
## The Hidden Markov Model
## A simple 1D Kalman Filter
## A complete Kalman Filter
## The Extended Kalman Filter
## The Basic Particle Filter
## A Tracking Particle Filter
## The Markov Random Field for Image Denoising
## A demonstration of finding paths in graphs
## An image for denoising
## Chapter 17 (Symmetric Weights and Deep Belief Networks):
## A Hopfield network
## The Restricted Boltzmann Machine
## The Deep Belief Network Algorithm
## Chapter 18 (Gaussian Processes):
## The Gaussian Process for Regression Algorithm
## The Gaussian Process for Classification Algorithm
## Demo of the Gaussian Process for Classification
## Plots of the Weibull and Gaussian distributions (Fig 18.1)
## Plots of GP samples (Fig 18.2)
## Very simple data for the Gaussian Process Regression demo
## Datasets
## Many of the datasets used in the book are available from the UCI Machine Learning Repository. In particular, look for the Iris data, the Pima Indian data, the car safety data, the auto-mpg data, the wine data, and the mushroom data.#

## Two of the most popular machine learning demonstration datasets are the MNIST set of zip code digits, which is available here, and the binary alpha digits dataset, which can be downloaded here. Finally, there are a couple of smaller datasets that are not available elsewhere, at least in their current form, and so should be downloaded from this website:
## The Palmerston North Ozone dataset
## Training data for the prostate dataset
## Test data for the prostate dataset (variables are log cancer volume, log prostate weight, age, lbph, svi, lcp, Gleason score, pgg45 and the last one is response lpsa)
## The Ruapehu dataset (thanks to Mark Bebbington)
## Short version of the e. coli dataset#
